[TOC]

# ç®€å•æ˜æœ—çš„ RNN å†™è¯—æ•™ç¨‹

æœ¬æ¥æƒ³åšä¸€ä¸ªæ ‡é¢˜å…šçš„ï¼Œå–äº†ä¸€ä¸ª**å²ä¸Šæœ€ç®€å•çš„ RNN å†™è¯—æ•™ç¨‹**è¿™æ ‡é¢˜ï¼Œä½†æ˜¯åæ¥æƒ³äº†æƒ³ï¼Œè¿™TMä¸å°±æ˜¯æ ‡é¢˜å…šå—ï¼Ÿæ€ä¹ˆæ´»æˆäº†è‡ªå·±æœ€è®¨åŒçš„æ¨¡æ ·ï¼ŸğŸ˜’åæ¥å°±æ”¹æˆäº†è¿™ä¸ªæ ‡é¢˜ã€‚

åœ¨ä¸Šç¯‡åšå®¢[ç½‘ç»œæµé‡é¢„æµ‹å…¥é—¨ï¼ˆä¸€ï¼‰ä¹‹RNN ä»‹ç»](https://www.cnblogs.com/xiaohuiduan/p/14324502.html)ä¸­ï¼Œä»‹ç»äº†RNNçš„åŸç†ï¼Œè€Œåœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œå°†ä»‹ç»å¦‚ä½•ä½¿ç”¨kerasæ„å»ºRNNï¼Œç„¶åè‡ªåŠ¨å†™è¯—ã€‚

é¡¹ç›®åœ°å€ï¼š[Githubï¼šhttps://github.com/xiaohuiduan/rnn_chinese_poetry](https://github.com/xiaohuiduan/rnn_chinese_poetry)

![](imgs/rnn_poetry.jpg)



## æ•°æ®é›†ä»‹ç»

æ—¢ç„¶æ˜¯å†™è¯—ï¼Œå½“ç„¶å¾—æœ‰æ•°æ®é›†ï¼Œä¸è¿‡è¿˜å¥½æœ‰å¤§ç¥å·²ç»å°†æ•°æ®é›†å‡†å¤‡å¥½äº†ï¼Œå…·ä½“æ•°æ®é›†çš„æ¥æºå·²ä¸å¯çŸ¥ï¼Œå› ä¸ºç½‘ä¸ŠåŸºæœ¬ä¸Šéƒ½æ˜¯ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†ã€‚*ï¼ˆå¦‚æœæœ‰äººçŸ¥é“ï¼Œå¯ä»¥åœ¨è¯„è®ºåŒºæŒ‡å‡ºï¼Œç„¶åæˆ‘å†æ·»åŠ ä¸Šï¼‰*

æ•°æ®é›†åœ°å€ï¼š[Github](https://github.com/xiaohuiduan/rnn_chinese_poetry/blob/main/data/poetry.txt)ï¼Œæ•°æ®é›†éƒ¨åˆ†æ•°æ®å¦‚ä¸‹æ‰€ç¤ºï¼š

![](imgs/image-20210125151810940.png)

åœ¨æ•°æ®é›†ä¸­ï¼Œ**æ¯ä¸€è¡Œ**éƒ½æ˜¯ä¸€é¦–å”è¯—ï¼Œå…¶ä¸­ï¼Œè¯—çš„é¢˜ç›®å’Œå†…å®¹ä»¥ **":"** åˆ†å¼€ï¼Œæ¯ä¸€é¦–è¯—éƒ½æœ‰é¢˜ç›®ï¼Œä½†æ˜¯ä¸ä¸€å®šæœ‰å†…å®¹ï¼ˆä¹Ÿå°±æ˜¯è¯´å†…å®¹å¯èƒ½ä¸ºç©ºï¼‰ã€‚å…¶ä¸­ï¼Œè¯—å†…å®¹ä¸­çš„æ ‡ç‚¹ç¬¦å·éƒ½æ˜¯**å…¨è§’ç¬¦å·**ã€‚æœ‰ä¸€äº›è¯—äº”è¨€è¯—ï¼Œä¸è¿‡ä¹Ÿæœ‰ä¸€äº›è¯—ä¸æ˜¯äº”è¨€çš„ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬åªè€ƒè™‘**äº”è¨€è¯—**ï¼ˆå¤§æ¦‚æœ‰27ké¦–ï¼‰ã€‚



## ä»£ç æ€è·¯

### è¾“å…¥ and è¾“å‡º

é¦–å…ˆæˆ‘ä»¬å¾—å…ˆå¼„æ¸…æˆ‘ä»¬è¦å¹²ä»€ä¹ˆï¼Œç„¶åæ‰èƒ½æ›´å¥½å¾—å†™ä»£ç ã€‚å¦‚æ ‡é¢˜æ‰€ç¤ºï¼Œç›®çš„æ˜¯ä½¿ç”¨RNNå†™è¯—ï¼Œé‚£ä¹ˆå¿…ç„¶æœ‰è¾“å…¥å’Œè¾“å‡ºã€‚é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼ŒRNNçš„è¾“å…¥æ˜¯ä»€ä¹ˆï¼Œè¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬å¸Œæœ›rnnèƒ½å¤Ÿå†™è¯—ï¼Œé‚£ä¹ˆæ€ä¹ˆå†™å‘¢ï¼Ÿæˆ‘ä»¬è¿™æ ·å®šä¹‰å¦‚ä¸‹çš„æ–¹å¼ï¼š

![](imgs/rnn_io.svg)

RNNæ¥å— **6ä¸ªå­—ç¬¦**ï¼ˆ5ä¸ªå­—+ä¸€ä¸ªæ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œç„¶åè¾“å‡ºä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚è‡³äºæ€ä¹ˆç”Ÿæˆä¸€é¦–å®Œæ•´çš„è¯—è¯ï¼Œç­‰åˆ°åé¢è®¨è®ºã€‚

RNNå½“ç„¶ä¸èƒ½å¤Ÿç›´æ¥æ¥å— "åºŠå‰æ˜æœˆå…‰ï¼Œ" è¿™ä¸ªä¸­æ–‡çš„è¾“å…¥ï¼Œæˆ‘ä»¬è¦å¯¹å…¶è¿›è¡Œ Encodeï¼Œå˜æˆæ•°å­—ï¼Œç„¶åæ‰èƒ½å¤Ÿè¾“å…¥åˆ°RNNç½‘ç»œä¸­ã€‚åŒç†ï¼ŒRNNè¾“å‡ºçš„è‚¯å®šä¹Ÿä¸æ˜¯ä¸€ä¸ªä¸­æ–‡å­—ç¬¦ï¼Œæˆ‘ä»¬ä¹Ÿè¦å¯¹å…¶è¿›è¡ŒDecode æ‰èƒ½å°†è¾“å‡ºå˜æˆä¸€ä¸ªä¸­æ–‡å­—ç¬¦ã€‚

æ€ä¹ˆè¿›è¡ŒEncodeï¼Œæœ‰ä¸€ä¸ªå¾ˆç®€å•çš„æ–¹æ³•ï¼Œé‚£å°±æ˜¯è¿›è¡Œ**one-hot**ç¼–ç ï¼Œå¯¹äºæ¯ä¸€ä¸ªå­—ï¼ˆåŒ…æ‹¬æ ‡ç‚¹ç¬¦å·åœ¨å†…ï¼‰æˆ‘ä»¬éƒ½è¿›è¡Œonehotç¼–ç ï¼Œè¿™æ ·å°±å¯ä»¥äº†ã€‚ä½†å®é™…ä¸Šï¼Œè¿™ä¸ªè¿™æ ·ä¼šæœ‰ä¸€ç‚¹å°é—®é¢˜ã€‚åœ¨æ•°æ®é›†ä¸­ï¼Œæ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„è¯—ï¼Œå¤§æ¦‚ç”±è¿‘ **7,000** ä¸ªå­—ç¬¦ç»„æˆï¼Œå¦‚æœå¯¹æ¯ä¸€ä¸ªå­—éƒ½è¿›è¡Œonehotç¼–ç çš„è¯ï¼Œå°±ä¼šæ¶ˆè€—å¤§é‡çš„å†…å­˜ï¼ŒåŒæ—¶ä¹Ÿä¼šåŠ å¤§è®¡ç®—çš„å¤æ‚åº¦ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å®šä¹‰å¦‚ä¸‹ï¼šåªå¯¹å‰å‡ºç°é¢‘ç‡æœ€å¤šçš„ **2999**  ä¸ªå­—ç¬¦è¿›è¡Œ one-hot ç¼–ç ï¼Œå¯¹äºå‰©ä¸‹çš„å­—ï¼Œç”¨ **â€œ â€**ï¼ˆç©ºæ ¼å­—ç¬¦ï¼‰ä»£æ›¿ã€‚è¿™æ ·ä¸€å…±åªéœ€è¦å¯¹3000ä¸ªå­—ç¬¦è¿›è¡Œone-hotç¼–ç å°±ğŸ†—äº†ï¼ˆ2999ä¸ªå­—ç¬¦+ä¸€ä¸ªç©ºæ ¼å­—ç¬¦ï¼‰ã€‚

![](imgs/RNN_one-hot-io-1611562307620.svg)



### è®­ç»ƒé›†æ„å»º

åœ¨å‰é¢æˆ‘ä»¬å®šä¹‰äº†RNNçš„è¾“å…¥å’Œè¾“å‡ºï¼ŒåŒæ—¶ä¹Ÿæœ‰è¯—çš„æ•°æ®é›†ï¼Œ é‚£ä¹ˆæˆ‘ä»¬æ„å»º**è®­ç»ƒé›†**å‘¢ï¼Ÿå‚è€ƒ[RNNæ¨¡å‹ä¸NLPåº”ç”¨(6/9)ï¼šText Generation (è‡ªåŠ¨æ–‡æœ¬ç”Ÿæˆ)](https://www.youtube.com/watch?v=10cjvcrU_ZU&list=PLvOO0btloRnuTUGN4XqO85eKPeFSZsEqK&index=6&ab_channel=ShusenWang)

å…·ä½“æ­¥éª¤å¦‚ä¸‹å›¾æ‰€ç¤ºï¼šæˆ‘ä»¬å°†ä¸€å¥è¯—å¯ä»¥è¿›è¡Œå¦‚ä¸‹åˆ‡åˆ†ã€‚ç„¶åå°†åˆ‡åˆ†å¾—åˆ°çš„æ•°æ®è¿›è¡Œone-hotç¼–ç ï¼Œç„¶åè¿›è¡Œè®­ç»ƒå³å¯ã€‚ï¼ˆè¿™æ ·çœ‹æ¥ï¼Œæ¯ä¸€é¦–è¯—å¯ä»¥ç”Ÿæˆå¾ˆå¤šçš„æ•°æ®é›†ï¼‰

![](imgs/RNN_split_data (1).svg)



### ç”Ÿæˆä¸€é¦–å®Œæ•´çš„è¯—

å‰é¢æˆ‘ä»¬è®¨è®ºäº†å…³äºç½‘ç»œçš„è¾“å…¥å’Œè¾“å‡ºï¼Œä»¥åŠæ•°æ®é›†çš„æ„å»ºï¼Œé‚£ä¹ˆï¼Œå‡å¦‚æˆ‘ä»¬æœ‰ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¦‚ä½•æ¥äº§ç”Ÿä¸€é¦–è¯—çš„ï¼Ÿ

ç”Ÿæˆä¸€é¦–å®Œæ•´çš„è¯—çš„æµç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼Œä¸è®­ç»ƒçš„æ“ä½œæœ‰ç‚¹ç±»ä¼¼ï¼Œåªä¸è¿‡ä¼šå°†RNNçš„è¾“å‡ºé‡æ–°å½“ä½œRNNçš„è¾“å…¥ã€‚ï¼ˆä»¥æ­¤æ¥äº§ç”Ÿç¬¦åˆå­—æ•°è¦æ±‚çš„è¯—ï¼‰

![](imgs/create_whole_poem.svg)



ç»è¿‡ä¸Šè¿°çš„æ“ä½œï¼Œå¤§å®¶å®é™…ä¸Šå¯ä»¥å°è¯•çš„å†™ä¸€äº›ä»£ç äº†ï¼ŒåŸºæœ¬ä¸Šä¸ä¼šæœ‰å¾ˆå¤§çš„é—®é¢˜ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘å°†è®²ä¸€è®²å…·ä½“æ€ä¹ˆå®ç°ã€‚



## ä»£ç å®ç°

é¦–å…ˆå®šä¹‰ä¸€äº›é…ç½®ï¼š

- DISALLOWED_WORDSï¼šå¦‚æœåœ¨è¯—ä¸­å‡ºç°äº†**DISALLOWED_WORDS**ï¼Œåˆ™èˆå¼ƒè¿™é¦–è¯—ã€‚

```python
# è¯—dataçš„åœ°å€
poetry_data_path = "./data/poetry.txt"
# å¦‚æœè¯—è¯ä¸­å‡ºç°è¿™äº›è¯ï¼Œåˆ™å°†è¯—èˆå¼ƒ
DISALLOWED_WORDS = ['ï¼ˆ', 'ï¼‰', '(', ')', '__', 'ã€Š', 'ã€‹', 'ã€', 'ã€‘', '[', ']']
# å–3000ä¸ªå­—ä½œè¯—,å…¶ä¸­åŒ…æ‹¬ç©ºæ ¼å­—ç¬¦
WORD_NUM = 3000
# å°†å‡ºç°å°‘çš„å­—ä½¿ç”¨ç©ºæ ¼ä»£æ›¿
UNKONW_CHAR = " "
# æ ¹æ®å‰6ä¸ªå­—é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ï¼Œæ¯”å¦‚è¯´æ ¹æ®â€œå¯’éšç©·å¾‹å˜ï¼Œâ€é¢„æµ‹â€œæ˜¥â€
TRAIN_NUM = 6
```



### è¯»å–æ–‡ä»¶

é’ˆå¯¹äºæ•°æ®é›†ï¼Œæˆ‘ä»¬æœ‰å¦‚ä¸‹çš„è¦æ±‚ï¼š

- å¿…é¡»æ˜¯**äº”è¨€è¯—**ï¼ˆä¸è¿‡ä¸‹é¢çš„ä»£ç æ— æ³•å®Œå…¨ä¿è¯æ˜¯äº”è¨€è¯—ï¼‰ï¼ŒåŒæ—¶è‡³å°‘è¦æœ‰**ä¸¤å¥è¯—**
- ä¸èƒ½å‡ºç°ä¸Šæ–‡ä¸­å®šä¹‰çš„DISALLOWED_WORDS

å‰é¢æˆ‘ä»¬è¯´äº†ï¼Œæ¯ä¸€é¦–è¯—å¿…æœ‰é¢˜ç›®å’Œå†…å®¹ï¼ˆå†…å®¹å¯ä»¥ä¸ºç©ºï¼‰ï¼Œå…¶ä¸­ï¼Œé¢˜ç›®å’Œå†…å®¹ä»¥ ":"ï¼ˆåŠè§’ï¼‰åˆ†å¼€ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ `line.split(":")[1]`è·å¾—è¯—çš„å†…å®¹ã€‚

ä¸‹è¿°ä»£ç å®ç°äº†ä¸¤ä¸ªåŠŸèƒ½ï¼š

1. è·å¾—ç¬¦åˆè¦æ±‚çš„è¯—ï¼š`(len(poetry)-1) % 6`ï¼Œæ¯ä¸€é¦–äº”è¨€è¯—ï¼ŒåŒ…æ‹¬â€œï¼Œã€‚â€ä¸€å…±æœ‰$6*n$ ä¸ªå­—ï¼ŒåŒæ—¶æ¯ä¸€é¦–è¯—æ˜¯ä»¥ "\n" ç»“å°¾çš„ï¼Œå› ä¸ºæˆ‘ä»¬``(len(poetry)-1)%6==0``åˆ™å°±ä»£è¡¨ç¬¦åˆè¦æ±‚ã€‚åŒæ—¶äº”è¨€è¯—çš„ç¬¬6ä¸ªå­—ç¬¦æ˜¯"ï¼Œ"â€”â€”> ä½¿ç”¨`poetrys`ä¿å­˜ã€‚
2. è·å¾—è¯—ä¸­å‡ºç°çš„å­—ç¬¦ã€‚â€”â€”>ä½¿ç”¨`all_word`ä¿å­˜ã€‚

```python
# ä¿å­˜è¯—è¯
poetrys = []
# ä¿å­˜åœ¨è¯—è¯ä¸­å‡ºç°çš„å­—
all_word = []

with open(poetry_data_path,encoding="utf-8") as f:
    for line in f:
        # è·å¾—è¯—çš„å†…å®¹
        poetry = line.split(":")[1].replace(" ","")
        flag = True
        # å¦‚æœåœ¨å¥å­ä¸­å‡ºç°'ï¼ˆ', 'ï¼‰', '(', ')', '__', 'ã€Š', 'ã€‹', 'ã€', 'ã€‘', '[', ']'åˆ™èˆå¼ƒ
        for dis_word in DISALLOWED_WORDS:
            if dis_word in poetry:
                flag = False
                break

        # åªéœ€è¦5è¨€çš„è¯—ï¼ˆä¸¤å¥è¯—åŒ…æ‹¬æ ‡ç‚¹ç¬¦å·å°±æ˜¯12ä¸ªå­—ï¼‰ï¼Œå‡å¦‚å°‘äºä¸¤å¥è¯—åˆ™èˆå¼ƒ
        if  len(poetry) < 12 or poetry[5] != 'ï¼Œ' or (len(poetry)-1) % 6 != 0:
            flag = False

        if flag:
            # ç»Ÿè®¡å‡ºç°çš„è¯
            for word in poetry:
                all_word.append(word)
            poetrys.append(poetry)
```

![](imgs/image-20210125203730771.png)



### ç»Ÿè®¡å­—æ•°

å‰é¢æˆ‘ä»¬è¯´è¿‡ï¼Œåœ¨æ•°æ®é›†ä¸­ï¼Œæ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„è¯—ï¼Œå¤§æ¦‚ç”±è¿‘ **7,000** ä¸ªå­—ç»„æˆï¼Œå¦‚æœå¯¹æ¯ä¸€ä¸ªå­—éƒ½è¿›è¡Œone-hotç¼–ç çš„è¯ï¼Œå°±ä¼šæµªè´¹å¤§é‡çš„å†…å­˜ï¼ŒåŠ å¤§è®¡ç®—çš„å¤æ‚åº¦ã€‚è§£å†³æ–¹æ³•å¯ä»¥è¿™æ ·åšï¼š

> ä½¿ç”¨Counterå¯¹å­—æ•°è¿›è¡Œç»Ÿè®¡ï¼Œç„¶åæ ¹æ®å‡ºç°çš„æ¬¡æ•°è¿›è¡Œæ’åºï¼Œæœ€åå¾—åˆ°å‡ºç°é¢‘ç‡æœ€å¤šçš„2999ä¸ªå­—ã€‚

```python
from collections import Counter
# å¯¹å­—æ•°è¿›è¡Œç»Ÿè®¡
counter = Counter(all_word)
# æ ¹æ®å‡ºç°çš„æ¬¡æ•°ï¼Œè¿›è¡Œä»å¤§åˆ°å°çš„æ’åº
word_count = sorted(counter.items(),key=lambda x : -x[1])
most_num_word,_ = zip(*word_count)
# å–å‰2999ä¸ªå­—ï¼Œç„¶ååœ¨æœ€ååŠ ä¸Š" "
use_words = most_num_word[:WORD_NUM - 1] + (UNKONW_CHAR,)
```

![](imgs/image-20210125203759955.png)



### æ„å»ºword ä¸ idçš„æ˜ å°„

æˆ‘ä»¬éœ€è¦å¯¹wordè¿›è¡Œonehotç¼–ç ï¼Œæ€ä¹ˆç¼–å‘¢ï¼Ÿå¾ˆç®€å•ï¼Œæ¯ä¸€ä¸ªwordå¯¹åº”ä¸€ä¸ªidï¼Œç„¶åå¯¹è¿™ä¸ªidè¿›è¡Œone-hotç¼–ç å°±è¡Œäº†ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦æ„å»ºwordåˆ°idçš„æ˜ å°„ã€‚

> ä¸¾ä¸ªä¾‹å­ï¼šå¦‚æœä¸€å…±åªæœ‰3ä¸ªå­—â€œå”â€ï¼Œâ€œå®‹â€ï¼Œâ€œæ˜â€ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥æ„å»ºå¦‚ä¸‹çš„æ˜ å°„ï¼š
>
> "å”" â€”â€”> 0 ï¼›"å®‹"â€”â€”>1ï¼›"æ˜"â€”â€”>2ï¼›è¿›è¡Œone-hotç¼–ç åï¼Œåˆ™å°±å˜æˆäº†ï¼š
>
> - å”ï¼š[1,0,0]
> - å®‹ï¼š[0,1,0]
> - æ˜ï¼š[0,0,1]

æ„å»ºwordä¸idçš„æ˜ å°„æ˜¯å¿…é¡»çš„ï¼Œç»è¿‡å¦‚ä¸‹ç®€å•çš„ä»£ç ï¼Œä¾¿æ„æˆäº†æ˜ å°„ã€‚

```python
# word åˆ° idçš„æ˜ å°„ {'ï¼Œ': 0,'ã€‚': 1,'\n': 2,'ä¸': 3,'äºº': 4,'å±±': 5,â€¦â€¦}
word_id_dict = {word:index for index,word in enumerate(use_words)}

# id åˆ° wordçš„æ˜ å°„ {0: 'ï¼Œ',1: 'ã€‚',2: '\n',3: 'ä¸',4: 'äºº',5: 'å±±',â€¦â€¦}
id_word_dict = {index:word for index,word in enumerate(use_words)}
```

![](imgs/image-20210125195532009.png)



### è½¬æˆone-hotä»£ç 

ä¸‹é¢å®šä¹‰ä¸¤ä¸ªå‡½æ•°ï¼š

- word_to_one_hotå°†ä¸€ä¸ªå­—è½¬æˆone-hot å½¢å¼

  ![](imgs/image-20210125200034659.png)

- phrase_to_one_hot å°†ä¸€ä¸ªå¥å­è½¬æˆone-hotå½¢å¼

  ![](imgs/image-20210125200115940.png)

```python
import numpy as np
def word_to_one_hot(word):
    """å°†ä¸€ä¸ªå­—è½¬æˆonehotå½¢å¼

    :param word: [ä¸€ä¸ªå­—]
    :type word: [str]
    """
    one_hot_word = np.zeros(WORD_NUM)
    # å‡å¦‚å­—æ˜¯ç”Ÿåƒ»å­—ï¼Œåˆ™å˜æˆç©ºæ ¼
    if word not in word_id_dict.keys():
        word = UNKONW_CHAR
    index = word_id_dict[word]
    one_hot_word[index] = 1
    return one_hot_word

def phrase_to_one_hot(phrase):
    """å°†ä¸€ä¸ªå¥å­è½¬æˆonehot

    :param phrase: [ä¸€ä¸ªå¥å­]
    :type poetry: [str]
    """
    one_hot_phrase = []
    for word in phrase:
        one_hot_phrase.append(word_to_one_hot(word))
    return one_hot_phrase
```

### éšæœºæ‰“ä¹±æ•°æ®

```python
np.random.shuffle(poetrys)
```

### æ„å»ºè®­ç»ƒé›†

ç„¶åæˆ‘ä»¬éœ€è¦è¿›è¡Œå¦‚ä¸‹æ“ä½œï¼Œæ ¹æ®è¯—æ„å»ºæ•°æ®é›†(one-hotç¼–ç ä¹‹å‰çš„æ•°æ®é›†)ã€‚

![](imgs/RNN_split_data (1).svg)

æ„å»ºæ•°æ®é›†çš„æ—¶å€™æˆ‘ä»¬éœ€è¦æ³¨æ„ä¸€ä»¶äº‹æƒ…ï¼Œéœ€è¦åŒºåˆ†ä¸åŒçš„è¯—ï¼ˆå› ä¸ºæˆ‘ä»¬æ€»ä¸å¯èƒ½ç”¨Açš„è¯—å»é¢„æµ‹Bçš„è¯—å™»ï¼Œhhhï¼‰ã€‚æ¯ä¸€é¦–è¯—éƒ½æ˜¯ä»¥ "\n" ç»“å°¾çš„ï¼Œå› æ­¤ï¼Œå½“å¾ªç¯åˆ°"\n"æ—¶ï¼Œå°±ä»£è¡¨å¯¹äºè¿™é¦–è¯—ï¼Œæˆ‘ä»¬å·²ç»æ„å»ºå¥½æ•°æ®é›†äº†ï¼ˆä¸Šå›¾ä¸­çš„X_Dataã€ç”¨`X_train_word`è¡¨ç¤ºã€‘ï¼ŒY_Dataã€ç”¨`Y_train_word`è¡¨ç¤ºã€‘ï¼‰ã€‚

```python
X_train_word = []
Y_train_word = []

for poetry in poetrys:
    for i in range(len(poetry)):
        X = poetry[i:i+TRAIN_NUM]
        Y = poetry[i+TRAIN_NUM]
        if "\n" not in X and "\n" not in Y:
            X_train_word.append(X)
            Y_train_word.append(Y)
        else:
            break
```

åœ¨æ²¡æœ‰æ‰“ä¹±é¡ºåºçš„æƒ…å†µä¸‹ï¼Œéƒ¨åˆ†ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

![](imgs/image-20210125204257815.png)



### æ„å»ºæ¨¡å‹

ä½¿ç”¨çš„æ¡†æ¶ï¼š

- kerasï¼š2.4.3ï¼šå¦‚æœæƒ³ä½¿ç”¨æˆ‘è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·ä¿æŒç‰ˆæœ¬ä¸€è‡´ã€‚å¦‚æœè‡ªå·±è®­ç»ƒçš„è¯ï¼Œå°±æ— æ‰€è°“äº†ã€‚

æ¨¡å‹å›¾å¦‚ä¸‹æ‰€ç¤ºï¼Œæ¨¡å‹ç»“æ„å‚è€ƒ[Poems_generator_Keras](https://github.com/youyuge34/Poems_generator_Keras)ï¼Œå…³äºSimpleRNNçš„ä»‹ç»å¯ä»¥å‚è€ƒ[Keras-SimpleRNN](https://kldivergence.github.io/keras-docs-zh/layers/recurrent/#simplernn)ï¼Œå…³äºå¦‚ä½•ä½¿ç”¨kerasæ„å»ºç¥ç»ç½‘ç»œå¯ä»¥å‚è€ƒ[æ•°æ®æŒ–æ˜å…¥é—¨ç³»åˆ—æ•™ç¨‹ï¼ˆåä¸€ï¼‰ä¹‹keraså…¥é—¨ä½¿ç”¨ä»¥åŠæ„å»ºDNNç½‘ç»œè¯†åˆ«MNIST](https://www.cnblogs.com/xiaohuiduan/p/12806241.html)ã€‚

![](imgs/rnn_poetry_model.png)

åœ¨å‰é¢è¯´äº†ï¼ŒRNNæ¨¡å‹è¾“å…¥çš„æ˜¯ä¸€ä¸ª **6ä¸ªå­—ç¬¦** çš„å¥å­ï¼Œå› æ­¤ç»è¿‡one-hotç¼–ç åå°±ä¼šå˜æˆshapeä¸º(6,3000)çš„æ•°ç»„ï¼Œè€Œè¾“å‡ºä¸º**ä¸€ä¸ª**å­—ç¬¦ï¼Œå¯¹åº”one-hotç¼–ç çš„shapeä¸º(3000)ã€‚

![](imgs/RNN_one-hot-io-1611562307620.svg)

ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import keras
from keras.callbacks import LambdaCallback,ModelCheckpoint
from keras.models import Input, Model
from keras.layers import  Dropout, Dense,SimpleRNN 
from keras.optimizers import Adam
from keras.utils import plot_model

def build_model():
    print('building model')
    # è¾“å…¥çš„dimension
    input_tensor = Input(shape=(TRAIN_NUM,WORD_NUM))
    rnn = SimpleRNN(512,return_sequences=True)(input_tensor)
    dropout = Dropout(0.6)(rnn)

    rnn = SimpleRNN(256)(dropout)
    dropout = Dropout(0.6)(rnn)
    dense = Dense(WORD_NUM, activation='softmax')(dropout)

    model = Model(inputs=input_tensor, outputs=dense)
    optimizer = Adam(lr=0.001)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    model.summary()
    # ç”»å‡ºæ¨¡å‹å›¾
    # plot_model(model, to_file='model.png', show_shapes=True, expand_nested=True, dpi=500)
    return  model
```

å¯¹äºSimpleRNNï¼Œå¦‚æœ`return_sequences=True`ï¼Œåˆ™ä»£è¡¨å…¶è¿”å›å¦‚ä¸‹ï¼š

![](imgs/rnn-many-to-many-same-ltr.png)

å¦‚æœ`return_sequences=False`(é»˜è®¤)ï¼Œåˆ™ä»£è¡¨è¿”å›å¦‚ä¸‹æ‰€ç¤ºï¼š

![](imgs/rnn-many-to-one-ltr.png)

è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œå¥—äº†ä¸¤å±‚RNNã€‚

```python
model = build_model()
```



### æ‰¹åŠ è½½æ•°æ®

è¿™æ¬¡æ•°æ®é›†æ¯”è¾ƒå¤§ï¼Œä¸€å…±æœ‰$1559196$ä»½æ•°æ®ï¼Œä¸€èˆ¬æ¥è¯´æ²¡æœ‰è¿™ä¹ˆå¤§å†…å­˜å°†å…¶æ‰€æœ‰çš„æ•°æ®ä¸€æ¬¡æ€§å…¨éƒ¨è½¬æˆone-hotå½¢å¼ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼šåœ¨è®­ç»ƒçš„æ—¶å€™æ‰å¼€å§‹åŠ è½½æ•°æ®ï¼Œæ¯ä¸€æ¬¡åªéœ€è¦åŠ è½½batch_sizeçš„æ•°æ®ï¼Œç„¶ååªéœ€è¦å°†batch_size å¤§å°çš„æ•°æ®è½¬æˆone-hotå½¢å¼ï¼Œç„¶åè¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåªéœ€è¦å°†batch-sizeçš„æ•°æ®è½¬æˆone-hotï¼Œå¯ä»¥å¤§å¤§å‡å°å†…å­˜æ¶ˆè€—ã€‚

soï¼Œä½¿ç”¨kerasè®­ç»ƒçš„æ—¶å€™ï¼Œä¸èƒ½ä½¿ç”¨fitï¼ˆå› ä¸ºfitéœ€è¦ä¸€æ¬¡å°†æ•°æ®é›†å…¨éƒ¨æ”¾å…¥RAMä¸­ï¼‰ï¼Œè€Œåº”è¯¥ä½¿ç”¨[fit_generator](https://kldivergence.github.io/keras-docs-zh/models/model/#fit_generator)ï¼Œå…³äºå…¶ä½¿ç”¨æ¨èçœ‹çœ‹ï¼š[Keras å¦‚ä½•ä½¿ç”¨fitå’Œfit_generator](https://blog.csdn.net/zwqjoy/article/details/88356094)

```python
import math
def get_batch(batch_size = 32):
    """æºæºä¸æ–­äº§ç”Ÿäº§ç”Ÿone-hotç¼–ç çš„è®­ç»ƒæ•°æ®

    :param batch_size: [ä¸€æ¬¡äº§ç”Ÿè®­ç»ƒæ•°æ®çš„å¤§å°], defaults to 32
    :type batch_size: int, optional
    :yield: [è¿”å›Xï¼ˆnp.array(X_train_batch)ï¼‰å’ŒYï¼ˆnp.array(Y_train_batch)ï¼‰]
    :rtype: [X.shapeä¸º(batch_size, 6, 3000) , Y.shapeæ•°æ®çš„shape(batch_size, 3000)]
    """
    # ç¡®å®šæ¯è½®æœ‰å¤šå°‘ä¸ªbatch
    steps = math.ceil(len(X_train_word) / batch_size)
    while True:
        for i in range(steps):
            X_train_batch = []
            Y_train_batch = []
            X_batch_datas = X_train_word[i*batch_size:(i+1)*batch_size]
            Y_batch_datas = Y_train_word[i*batch_size:(i+1)*batch_size]

            for x,y in zip(X_batch_datas,Y_batch_datas):
                X_train_batch.append(phrase_to_one_hot(x))
                Y_train_batch.append(word_to_one_hot(y))
            yield np.array(X_train_batch),np.array(Y_train_batch)
```



### è®­ç»ƒçš„è¿‡ç¨‹ä¸­ç”Ÿæˆè¯—å¥

åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥æ¯ç»è¿‡ä¸€å®šæ•°é‡çš„epochç”Ÿæˆä¸€é¦–è¯—ï¼Œç”Ÿæˆè¯—çš„æ“ä½œå¦‚ä¸‹ï¼š

![](imgs/create_whole_poem.svg)

åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œè°ƒç”¨`generate_sample_result`ï¼Œå³å¯äº§ç”Ÿäº”è¨€è¯—ï¼Œç„¶åå°†ç”Ÿæˆçš„è¯—å†™å…¥åˆ°`out/out.txt`ä¸­ã€‚

```python
def predict_next(x):
    """ æ ¹æ®Xé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦

    :param x: [è¾“å…¥æ•°æ®]
    :type x: [xçš„shapeä¸º(1,TRAIN_NUM,WORD_NUM)]
    :return: [æœ€å¤§æ¦‚ç‡å­—ç¬¦çš„ç´¢å¼•ï¼Œæœ‰å¯èƒ½ä¸ºä¸º2999ï¼Œä¹Ÿå°±æ˜¯é¢„æµ‹çš„å­—ç¬¦å¯èƒ½ä¸ºâ€œ â€]
    :rtype: [int]
    """
    predict_y = model.predict(x)[0]
    # è·å¾—æœ€å¤§æ¦‚ç‡çš„ç´¢å¼•
    index = np.argmax(predict_y)
    return index

def generate_sample_result(epoch, logs):
    """ç”Ÿæˆäº”è¨€è¯—

    :param epoch: [ç›®å‰æ¨¡å‹è®­ç»ƒçš„epoch]
    :type epoch: [int]
    :param logs: [æ¨¡å‹è®­ç»ƒæ—¥å¿—]
    :type logs: [list]
    """
    # æ¯ä¸ªepochéƒ½äº§ç”Ÿè¾“å‡º
    if epoch % 1 == 0:
        # æ ¹æ®â€œä¸€æœæ˜¥å¤æ”¹ï¼Œâ€ç”Ÿæˆè¯—
        predict_sen = "ä¸€æœæ˜¥å¤æ”¹ï¼Œ"
        predict_data = predict_sen
        # ç”Ÿæˆçš„4å¥äº”è¨€è¯—ï¼ˆ4 * 6 = 24ï¼‰
        while len(predict_sen) < 24:
            X_data = np.array(phrase_to_one_hot(predict_data)).reshape(1,TRAIN_NUM,WORD_NUM)
            # æ ¹æ®6ä¸ªå­—ç¬¦é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦
            y = predict_next(X_data)
            predict_sen = predict_sen+ id_word_dict[y]
            # â€œå¯’éšç©·å¾‹å˜ï¼Œâ€ â€”â€”> â€œéšç©·å¾‹å˜ï¼Œæ˜¥â€
            predict_data = predict_data[1:]+id_word_dict[y]
        # å°†æ•°æ®å†™å…¥æ–‡ä»¶    
        with open('out/out.txt', 'a',encoding='utf-8') as f:
            f.write(write_data+'\n')
```

### å¼€å§‹è®­ç»ƒ

åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œæ¯éš”ä¸€ä¸ªepochï¼Œéƒ½ä¼šå°†æ¨¡å‹è¿›è¡Œä¿å­˜ï¼Œæ¯ä¸ªepochå®Œæˆçš„æ—¶å€™ï¼Œéƒ½ä¼šè°ƒç”¨`generate_sample_result`ç”Ÿæˆè¯—ã€‚

```python
batch_size = 2048
model.fit_generator(
            generator=get_batch(batch_size),
            verbose=True,
            steps_per_epoch=math.ceil(len(X_train_word) / batch_size),
            epochs=1000000,
            callbacks=[
                ModelCheckpoint("poetry_model.hdf5",verbose=1,monitor='val_loss',period=1),
                # æ¯æ¬¡å®Œæˆä¸€ä¸ªepochä¼šè°ƒç”¨generate_sample_resultäº§ç”Ÿäº”è¨€è¯—
                LambdaCallback(on_epoch_end=generate_sample_result)
            ]
    )
```

å› ä¸ºæˆ‘çš„ç”µè„‘å°±æ˜¯ä¸€ä¸ªmx250å°æ°´ç®¡ï¼Œæˆ‘å°±æ”¾åœ¨kaggleä¸Šé¢è·‘äº†ï¼Œæ¯•ç«Ÿç™½å«–å®ƒä¸é¦™å—ï¼Ÿå¦‚æœå®åœ¨æƒ³è‡ªå·±è·‘ï¼Œä½†æ˜¯æœ‰æ²¡æœ‰æ¯”è¾ƒå¥½çš„GPUï¼Œå¯ä»¥å°è¯•å°†`len(X_train_word)`æ”¹æˆå…¶ä»–çš„æ•°ï¼Œæ¯”å¦‚è¯´â€œ100000â€ã€‚è¦åœ¨å¦‚ä¸‹çš„ä¸¤ä¸ªåœ°æ–¹æ”¹ï¼Œè¿™æ ·çš„è¯ï¼Œå¾ˆå¿«å°±å¯ä»¥å‡ºè®­ç»ƒçš„ç»“æœã€‚ï¼ˆè¿™æ ·ä¼šå¯¼è‡´è®­ç»ƒçš„æ—¶å€™æ— æ³•è¦†ç›–æ•´ä¸ªæ•°æ®é›†ã€‚ï¼‰

![](imgs/image-20210126123029328.png)



### è¯—è¯ç”Ÿæˆ

æˆ‘åœ¨[Github](https://github.com/xiaohuiduan/rnn_chinese_poetry)ä¸­æä¾›äº†è®­ç»ƒå¥½çš„[æ¨¡å‹ï¼ˆæ³¨æ„kerasç‰ˆæœ¬æ˜¯2.4.3ï¼‰](https://github.com/xiaohuiduan/rnn_chinese_poetry/blob/main/poetry_model.hdf5)ï¼Œåœ¨ **[test.ipynb](https://github.com/xiaohuiduan/rnn_chinese_poetry/blob/main/test.ipynb)** ä¸­æä¾›äº†å¦‚ä½•åŠ è½½æ¨¡å‹ç„¶åç”Ÿæˆè¯—å¥çš„æ–¹æ³•ï¼Œåœ¨è¿™é‡Œå°±ä¸èµ˜è¿°äº†ã€‚

æœ€åç®€å•çš„å±•ç¤ºä¸€ä¸‹ç”Ÿæˆçš„ç»“æœï¼ˆå®é™…ä¸Šæ¨¡å‹è®­ç»ƒçš„æ•ˆæœå¹¶ä¸æ˜¯å¾ˆå¥½ï¼ŒğŸ¤­ï¼‰ï¼š

> åšå®¢å›­ç‰›é€¼ï¼Œå·å¿ƒé’å±±äººã€‚é›¨æ°´ä¸ä¸‰åœ¨ï¼ŒèŠ±å»ä¸ç›¸æ²³ã€‚



## æ€»ç»“

åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œè¯¦ç»†ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨kerasæ¨¡å‹æ„å»ºä¸€ä¸ªRNNæ¨¡å‹ï¼Œç„¶åä½¿ç”¨å…¶æ¥è‡ªåŠ¨ç”Ÿæˆäº”è¨€è¯—ã€‚å®é™…ä¸Šï¼Œåœ¨ä¸ªäººçœ‹æ¥ï¼Œä»£ç ä¸æ˜¯éš¾é¢˜ï¼Œæœ€éš¾çš„åº”è¯¥æ˜¯æ€è·¯ï¼Œå¦‚æœæ„å»ºä¸€ä¸ªæ¸…æ™°æ˜æœ—çš„æ€è·¯ï¼Œæ‰æ˜¯èƒ½å¤Ÿå†™å¥½ä»£ç çš„å‰æã€‚

é¡¹ç›®åœ°å€ï¼š[Github](https://github.com/xiaohuiduan/rnn_chinese_poetry)

### å‚è€ƒ

1. [RNNæ¨¡å‹ä¸NLPåº”ç”¨(6/9)ï¼šText Generation (è‡ªåŠ¨æ–‡æœ¬ç”Ÿæˆ)](https://www.youtube.com/watch?v=10cjvcrU_ZU&list=PLvOO0btloRnuTUGN4XqO85eKPeFSZsEqK&index=6&ab_channel=ShusenWang)
2. [Poems_generator_Keras](https://github.com/youyuge34/Poems_generator_Keras)
3. æ·±åº¦å­¦ä¹ æ¡†æ¶PyTorchï¼šå…¥é—¨ä¸å®è·µ
4. [ç”¨Keraså®ç°RNN+LSTMçš„æ¨¡å‹è‡ªåŠ¨ç¼–å†™å¤è¯—](https://www.ioiogoo.cn/2018/02/01/%E7%94%A8keras%E5%AE%9E%E7%8E%B0rnnlstm%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%87%AA%E5%8A%A8%E7%BC%96%E5%86%99%E5%8F%A4%E8%AF%97/)
5. [Keras-SimpleRNN](https://kldivergence.github.io/keras-docs-zh/layers/recurrent/#simplernn)
6. [Keras-fit_generator](https://kldivergence.github.io/keras-docs-zh/models/model/#fit_generator)
7. [æ•°æ®æŒ–æ˜å…¥é—¨ç³»åˆ—æ•™ç¨‹ï¼ˆåä¸€ï¼‰ä¹‹keraså…¥é—¨ä½¿ç”¨ä»¥åŠæ„å»ºDNNç½‘ç»œè¯†åˆ«MNIST](https://www.cnblogs.com/xiaohuiduan/p/12806241.html)
8. [Keras å¦‚ä½•ä½¿ç”¨fitå’Œfit_generator](https://blog.csdn.net/zwqjoy/article/details/88356094)

